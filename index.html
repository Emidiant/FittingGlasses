<html>
<head>
    <title>threejs - models</title>
    <link href="style.css" rel="stylesheet">
</head>
<body>
    <script src="libs/face-api.min.js"></script>

    <video id="video" width="720" height="560" autoplay muted></video>
    <canvas id="canvas" width="720" height="560" class="d-none"></canvas>

    <script src="libs/three.js"></script>
    <script src="libs/GLTFLoader.js"></script>
    <script src="keyboard.js"></script>


    <script>
        const video = document.getElementById('video');

        Promise.all([
            faceapi.nets.tinyFaceDetector.loadFromUri('models/tiny_face_detector_model-weights_manifest.json'),
            faceapi.nets.faceLandmark68Net.loadFromUri('models/face_landmark_68_model-weights_manifest.json'),
            faceapi.nets.faceRecognitionNet.loadFromUri('models/face_recognition_model-weights_manifest.json')
        ]).then(startVideo)

        function startVideo() {
            navigator.getUserMedia(
                { video: {} },
                stream => video.srcObject = stream,
                err => console.error(err)
            )
        }

        var renderer,
            scene,
            camera,
            myCanvas = document.getElementById('canvas');

        //RENDERER
        renderer = new THREE.WebGLRenderer({
            canvas: myCanvas,
            antialias: true,
            transparent: true
        });
        renderer.setClearColor(0x000000, 0);
        renderer.setPixelRatio(window.devicePixelRatio);
        renderer.setSize(720, 560);

        //CAMERA
        camera = new THREE.PerspectiveCamera(50, window.innerWidth/window.innerHeight, 0.01, 5000);
        // camera.rotation.y = 45/180*Math.Pi;
        // camera.position.x = 800;
        camera.position.y = 30;
        camera.position.z = 70;

        //SCENE
        scene = new THREE.Scene();

        var vidTexture = new THREE.VideoTexture(video);
        scene.background = vidTexture;


        //LIGHTS
        var light = new THREE.AmbientLight(0xffffff, 0.5);
        scene.add(light);

        var light2 = new THREE.PointLight(0xffffff, 0.5);
        scene.add(light2);

        var loader = new THREE.GLTFLoader();

        loader.load('glasses_models/vuzix.glb', handle_load);

        var mesh;

        function handle_load(gltf) {

            console.log(gltf);
            mesh = gltf.scene;
            console.log(mesh.children[0]);
            mesh.children[0].material = new THREE.MeshLambertMaterial();
            scene.add( mesh );
            // mesh.position.z = 0;
        }


        //RENDER LOOP
        var delta = 0;
        var prevTime = Date.now();

        function render(x, y) {
            if (mesh) {
                mesh.position.y = -(y/560 * 50 - 25);
                mesh.position.x = x/720 * 100 - 50;  // необходимо разобраться с координатами
            }
            renderer.render(scene, camera);
            // requestAnimationFrame(render);
        }



        keyboard = new KeyboardState();

        function animate() {
            requestAnimationFrame( animate );
            if ( keyboard.pressed("W") )
                mesh.translateY( 0.1 );
            if ( keyboard.pressed("S") )
                mesh.translateY( - 0.1 );
            if ( keyboard.pressed("A") ){
                console.log("entered in A");
                mesh.translateX( - 0.1 );
            }
            if ( keyboard.pressed("D") ){
                console.log("entered in D");
                mesh.translateX( 0.1 );
            }
            if ( keyboard.pressed("F") ){ // press F to view current position 3d model
                console.log("mesh position " + mesh.position.x + " " + mesh.position.y)
            }

            renderer.render( scene, camera );
        }

        video.addEventListener('play', () => {
            const canvas = faceapi.createCanvasFromMedia(video)
            document.body.append(canvas)
            const displaySize = { width: video.width, height: video.height }
            faceapi.matchDimensions(canvas, displaySize)

            setInterval(async () => {
                const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks()
                const resizedDetections = faceapi.resizeResults(detections, displaySize)

                canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
                faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)

                const text = ['x = 0','y = 0']
                const anchor = { x: 0, y: 0 }
                const drawOptions = {
                    anchorPosition: 'TOP_LEFT',
                    backgroundColor: 'rgba(0, 0, 0, 0.5)'
                }
                const drawBox = new faceapi.draw.DrawTextField(text, anchor, drawOptions)
                drawBox.draw(canvas)

                const text2 = ['x = 720','y = 0']
                const anchor2 = { x: 700, y: 0 }
                const drawOptions2 = {
                    anchorPosition: 'TOP_LEFT',
                    backgroundColor: 'rgba(0, 0, 0, 0.5)'
                }
                const drawBox2 = new faceapi.draw.DrawTextField(text2, anchor2, drawOptions2)
                drawBox2.draw(canvas)


                if (resizedDetections) {
                    // console.log(JSON.stringify(resizedDetections))
                    // console.log(resizedDetections[0].landmarks._positions[36]._x)
                    // const leftEye = resizedDetections.getLeftEye() //36, 42
                    // const rightEye = resizedDetections.getRightEye() //42, 48
                    // const leftBrow = resizedDetections.getLeftEyeBrow()
                    // const righBrow = resizedDetections.getRightEyeBrow()
                    // const jawOutline = detections.getJawOutline()
                    // console.log("Left Eye landmarks ===========>" + Math.round(leftEye[0].x) + " " + Math.round(leftEye[0].y));
                    // console.log("Right Eye landmarks ===========>" + JSON.stringify(rightEye));
                    if (canvas.getContext) {
                        let ctx = canvas.getContext('2d');
                        for (let i = 0; i < 6; i++){
                            let left_x = Math.round(resizedDetections[0].landmarks._positions[36+i]._x);
                            let left_y = Math.round(resizedDetections[0].landmarks._positions[36+i]._y);
                            ctx.fillRect(left_x, left_y, 5, 5);
                            let right_x = Math.round(resizedDetections[0].landmarks._positions[42+i]._x);
                            let right_y = Math.round(resizedDetections[0].landmarks._positions[42+i]._y);
                            ctx.fillRect(right_x,right_y,5,5);
                            // console.log(left_x, left_y)
                        }
                    }

                    render(Math.round(resizedDetections[0].landmarks._positions[36]._x),Math.round(resizedDetections[0].landmarks._positions[36]._y));
                    // animate();
                }

            }, 300)
        })
    </script>

</body>
</html>
